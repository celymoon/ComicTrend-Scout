import csv
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# Configura√ß√£o do Chrome
chrome_options = Options()
chrome_options.add_argument("--headless")  
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")

# Criar o driver do Chrome
driver = webdriver.Chrome(options=chrome_options)

# URL da p√°gina inicial da Panini
url = "https://panini.com.br/planet-manga?product_list_order=ratings_summary"
driver.get(url)

# Esperar a p√°gina carregar completamente
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
)

time.sleep(5)

# Capturar o HTML
html_source = driver.page_source
soup = BeautifulSoup(html_source, "html.parser")

# ENCONTRAR O N√öMERO DA √öLTIMA P√ÅGINA
try:
    ultima_pagina_element = soup.find("a", class_="page last")
    if ultima_pagina_element:
        # Pegamos todos os <span> dentro do link
        spans = ultima_pagina_element.find_all("span")
        total_paginas = int(spans[-1].text.strip())  # Pegamos o √∫ltimo <span>, que tem o n√∫mero correto
    else:
        total_paginas = 1
except Exception as e:
    print(f"‚ö†Ô∏è Erro ao encontrar a √∫ltima p√°gina: {e}")
    total_paginas = 1

print(f"üîç Encontradas {total_paginas} p√°ginas para scraping.")

# üîπ Criar o arquivo CSV e escrever os cabe√ßalhos
with open("mangas_panini.csv", "w", newline="", encoding="utf-8") as csvfile:
    fieldnames = ["T√≠tulo", "Pre√ßo Original", "Desconto", "Disponibilidade", "Pr√©-venda"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    # üîÑ LOOP PARA NAVEGAR POR TODAS AS P√ÅGINAS AUTOMATICAMENTE
    for pagina in range(1, total_paginas + 1):
        url = f"https://panini.com.br/planet-manga?p={pagina}&product_list_order=ratings_summary"
        print(f"\nüìÑ Scraping da P√°gina {pagina}...")
        driver.get(url)

        # Esperar os produtos carregarem completamente
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
            )
        except:
            print(f" Produtos n√£o carregaram na p√°gina {pagina}. Pulando...")
            continue

        # Capturar o HTML da p√°gina carregada
        html_source = driver.page_source
        soup = BeautifulSoup(html_source, "html.parser")

        # Encontrar os produtos na p√°gina
        mangas = soup.find_all("li", class_="item product product-item")

        for manga in mangas:
            title = manga.find("a", class_="product-item-link")
            title_text = title.text.strip() if title else "T√≠tulo n√£o encontrado"

            
        # Encontrar pre√ßo com desconto (special-price ‚Üí span.price)
            special_price = manga.find("span", class_="special-price")
            special_price_text = special_price.find("span", class_="price").text.strip() if special_price and special_price.find("span", class_="price") else None

        # Encontrar pre√ßo original (old-price ‚Üí span.price)
            original_price = manga.find("span", class_="price-wrapper")
            original_price_text = original_price.find("span", class_="price").text.strip() if original_price and original_price.find("span", class_="price") else None
            old_price = manga.find("span", class_="old-price")
            old_price_text = old_price.find("span", class_="price").text.strip() if old_price and old_price.find("span", class_="price") else original_price_text
       
        #encontrar prevenda
            prevenda = manga.find("div", class_="amlabel-text")
            prevenda_text = prevenda.text.strip() if prevenda else None
        #encontrar disponibilidade
            status = manga.find("a", class_="bt-stock-unavailable")
            status_text = status.text.strip() if status else "Dispon√≠vel"

            print(f"{title_text}, {old_price_text}, {special_price_text}, {status_text}, {prevenda_text}")

            # **Salvar os dados no CSV**
            writer.writerow({
                "T√≠tulo": title_text,
                "Pre√ßo Original": old_price_text if old_price_text else "N√£o dispon√≠vel",
                "Desconto": special_price_text if special_price_text else "Sem desconto",
                "Disponibilidade": status_text if status_text else "Dispon√≠vel",
                "Pr√©-venda": prevenda_text if prevenda_text else "N√£o"
            })

# Fechar o navegador depois que tudo terminou
driver.quit()

print("\n‚úÖ Dados salvos em 'mangas_panini.csv' com sucesso!")
