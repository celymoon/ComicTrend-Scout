import os
import csv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time


# Configura√ß√£o do Chrome (sem necessidade de baixar o ChromeDriver)
chrome_options = Options()
chrome_options.add_argument("--headless")  # Para rodar sem abrir o navegador
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")

# Criar o driver do Chrome automaticamente
driver = webdriver.Chrome(options=chrome_options)

# URL da p√°gina da Panini
url = "https://panini.com.br/planet-manga?gad_source=1&p=4"
driver.get(url)

# Esperar at√© os produtos carregarem completamente (m√°x. 10 segundos)
try:
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
    )
except:
    print("Os produtos n√£o foram carregados completamente.")

# Capturar o HTML da p√°gina depois do carregamento din√¢mico
html_source = driver.page_source

# Usar BeautifulSoup para extrair informa√ß√µes
soup = BeautifulSoup(html_source, "html.parser")

# Encontrar os produtos na p√°gina
mangas = soup.find_all("li", class_="item product product-item")

# Criar o arquivo CSV e escrever os cabe√ßalhos
csv_path = os.path.join("Data", "mangas_panini.csv")
with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
    fieldnames = ["T√≠tulo", "Pre√ßo Original", "Desconto"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    for manga in mangas:
        # Encontrar t√≠tulo
        title = manga.find("a", class_="product-item-link")
        title_text = title.text.strip() if title else "T√≠tulo n√£o encontrado"
        manga_url = title['href'] if title else None


        # Encontrar pre√ßo com desconto (special-price ‚Üí span.price)
        special_price = manga.find("span", class_="special-price")
        special_price_text = special_price.find("span", class_="price").text.strip() if special_price else None

        # Encontrar pre√ßo original (old-price ‚Üí span.price)
        original_price = manga.find("span", class_="price-wrapper")
        original_price_text = original_price.find("span", class_="price").text.strip() if original_price else None
        old_price = manga.find("span", class_="old-price")
        old_price_text = old_price.find("span", class_="price").text.strip() if old_price else original_price_text

        print(f"üìö {title_text} - | Pre√ßo Original: {old_price_text} | Com Desconto: {special_price_text} | Link:{manga_url}")

        # Salvar os dados no CSV
        writer.writerow({
            "T√≠tulo": title_text,
            "Pre√ßo Original": old_price_text if old_price_text else "N√£o dispon√≠vel",
            "Desconto": special_price_text if special_price_text else "Sem desconto",
        })

# Fechar o navegador
driver.quit()

# print(f"‚úÖ Dados salvos com sucesso em: {csv_path}")

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time

# Configura√ß√£o do Selenium
chrome_options = Options()
chrome_options.add_argument("--headless")  # Executar em modo headless
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")

# Inicializar o driver do Selenium
driver = webdriver.Chrome(options=chrome_options)

# URL do produto
url = "https://panini.com.br/solo-leveling-03"
driver.get(url)

# Aguardar o carregamento completo da p√°gina
time.sleep(5)  # Ajuste o tempo conforme necess√°rio

# Obter o HTML da p√°gina
html = driver.page_source
soup = BeautifulSoup(html, "html.parser")

# Fechar o driver do Selenium
driver.quit()

# Dicion√°rio para armazenar os dados extra√≠dos
dados = {
    "Autor": None,
    "M√™s de Lan√ßamento": None,
    "Ano de Lan√ßamento": None
}

# Encontrar todas as c√©lulas <td> com a classe 'col data'
td_elements = soup.find_all("td", class_="col data")

# Iterar sobre os elementos encontrados para buscar os dados espec√≠ficos
for td in td_elements:
    data_th = td.get("data-th", "").strip()
    valor = td.get_text(strip=True)
    
    # Identificar o autor
    if data_th == "Autores":
        dados["Autor"] = valor
    
    if data_th == "M√™s":
        dados["M√™s de Lan√ßamento"] = valor
    
    if data_th  == "Ano de publica√ß√£o":
        dados["Ano de Lan√ßamento"] = valor

for chave, valor in dados.items():
    print(f"{chave}: {valor}")

# import os
# import csv
# import time
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup

# # Configura√ß√£o do Chrome
# chrome_options = Options()
# chrome_options.add_argument("--headless")  
# chrome_options.add_argument("--disable-gpu")
# chrome_options.add_argument("--no-sandbox")

# # Criar o driver do Chrome
# driver = webdriver.Chrome(options=chrome_options)

# # URL da p√°gina inicial da Panini
# url = "https://panini.com.br/planet-manga?product_list_order=ratings_summary"
# driver.get(url)

# # Esperar a p√°gina carregar completamente
# WebDriverWait(driver, 10).until(
#     EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
# )

# time.sleep(5)  # Ajuste o tempo conforme necess√°rio

# # Capturar o HTML
# html_source = driver.page_source
# soup = BeautifulSoup(html_source, "html.parser")

# # ENCONTRAR O N√öMERO DA √öLTIMA P√ÅGINA
# try:
#     ultima_pagina_element = soup.find("a", class_="page last")
#     if ultima_pagina_element:
#         spans = ultima_pagina_element.find_all("span")
#         total_paginas = int(spans[-1].text.strip())
#     else:
#         total_paginas = 1
# except Exception as e:
#     print(f"‚ö†Ô∏è Erro ao encontrar a √∫ltima p√°gina: {e}")
#     total_paginas = 1

# print(f"üîç Encontradas {total_paginas} p√°ginas para scraping.")

# # Criar o arquivo CSV e escrever os cabe√ßalhos
# csv_path = os.path.join("Data", "mangas_panini.csv")
# with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
#     fieldnames = ["T√≠tulo", "Pre√ßo Original", "Desconto", "Disponibilidade", "Pr√©-venda", "Autor", "Ano de Lan√ßamento", "M√™s de Lan√ßamento"]
#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
#     writer.writeheader()

#     # LOOP PARA NAVEGAR POR TODAS AS P√ÅGINAS AUTOMATICAMENTE
#     for pagina in range(1, total_paginas + 1):
#         url = f"https://panini.com.br/planet-manga?p={pagina}&product_list_order=ratings_summary"
#         print(f"\nüìÑ Scraping da P√°gina {pagina}...")
#         driver.get(url)

#         # Esperar os produtos carregarem completamente
#         try:
#             WebDriverWait(driver, 10).until(
#                 EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
#             )
#         except:
#             print(f"Produtos n√£o carregaram na p√°gina {pagina} . Pulando...")
#             continue

#         # Capturar o HTML da p√°gina carregada
#         html_source = driver.page_source
#         soup = BeautifulSoup(html_source, "html.parser")

#         # Encontrar os produtos na p√°gina
#         mangas = soup.find_all("li", class_="item product product-item")

#         for manga in mangas:
#             # Extrair as informa√ß√µes principais
#             title = manga.find("a", class_="product-item-link")
#             title_text = title.text.strip() if title else "T√≠tulo n√£o encontrado"
#             manga_url = title['href'] if title else None

#             # Pre√ßo com desconto
#             special_price = manga.find("span", class_="special-price")
#             special_price_text = special_price.find("span", class_="price").text.strip() if special_price else None

#             # Pre√ßo original
#             original_price = manga.find("span", class_="price-wrapper")
#             original_price_text = original_price.find("span", class_="price").text.strip() if original_price else None

#             # Status de disponibilidade
#             status = manga.find("a", class_="bt-stock-unavailable")
#             status_text = status.text.strip() if status else "Dispon√≠vel"

#             # Informa√ß√µes de pr√©-venda
#             prevenda = manga.find("div", class_="amlabel-text")
#             prevenda_text = prevenda.text.strip() if prevenda else None

            
#              # Acessar a p√°gina do manga para coletar informa√ß√µes adicionais
#             try:
#                 if manga_url:
#                     driver.get(manga_url)  # Corrigido para usar o href diretamente
#                     WebDriverWait(driver, 30).until(  # Aumentei o tempo para 30 segundos
#                         EC.presence_of_element_located((By.CLASS_NAME, "product-info"))
#                     )

#                     # Capturar a p√°gina do manga
#                     manga_html = driver.page_source
#                     manga_soup = BeautifulSoup(manga_html, "html.parser")

#                     # Coletar autor, ano e m√™s de lan√ßamento
#                     author_text = "Autor n√£o encontrado"
#                     year = "Ano n√£o encontrado"
#                     month = "M√™s n√£o encontrado"

#                     # Encontrar o tbody com as informa√ß√µes
#                     tbody = manga_soup.find("tbody")
#                     if tbody:
#                         rows = tbody.find_all("tr")
#                         for row in rows:
#                             th = row.find("th")
#                             td = row.find("td")
#                             if th and td:
#                                 data_th = th.get("data-th")
#                                 if data_th == "Autores":
#                                     author_text = td.text.strip()
#                                 if data_th == "Ano de publica√ß√£o":
#                                     year = td.text.strip()
#                                 if data_th == "M√™s":
#                                     month = td.text.strip()

#                     # Retornar √† p√°gina principal
#                     driver.back()
#                     WebDriverWait(driver, 10).until(
#                         EC.presence_of_element_located((By.CLASS_NAME, "product-item-info"))
#                     )
#             except Exception as e:
#                 print(f"‚ö†Ô∏è Erro ao acessar a p√°gina do manga {title_text}: {e}")
#                 author_text, year, month = "N√£o dispon√≠vel", "N√£o dispon√≠vel", "N√£o dispon√≠vel"

#             # Salvar os dados no CSV
#             writer.writerow({
#                 "T√≠tulo": title_text,
#                 "Pre√ßo Original": original_price_text if original_price_text else "N√£o dispon√≠vel",
#                 "Desconto": special_price_text if special_price_text else "Sem desconto",
#                 "Disponibilidade": status_text if status_text else "Dispon√≠vel",
#                 "Pr√©-venda": prevenda_text if prevenda_text else "N√£o",
#                 "Autor": author_text,
#                 "Ano de Lan√ßamento": year,
#                 "M√™s de Lan√ßamento": month
#             })

# # Fechar o navegador depois que tudo terminou
# driver.quit()

# print("\n‚úÖ Dados salvos em 'mangas_panini.csv' com sucesso!")
